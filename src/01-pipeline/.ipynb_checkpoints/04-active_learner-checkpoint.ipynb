{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.callbacks import ProgressBar\n",
    "from skorch.hf import HuggingfacePretrainedTokenizer\n",
    "from sqlalchemy.orm import Query\n",
    "\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.orm import joinedload\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import src\n",
    "import src.db.models.bert_data as bm\n",
    "from src.db.connect import make_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE='cuda'\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 2048)\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "\n",
    "# Choose a tokenizer and BERT model that work together\n",
    "# uncased version would be: \"dbmdz/bert-base-german-uncased\"\n",
    "TOKENIZER = \"bert-base-german-cased\"\n",
    "PRETRAINED_MODEL = \"bert-base-german-cased\"\n",
    "\n",
    "# model hyper-parameters\n",
    "OPTMIZER = torch.optim.AdamW\n",
    "LR = 5e-5\n",
    "MAX_EPOCHS = 10\n",
    "CRITERION = nn.CrossEntropyLoss\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "engine = make_engine(\"DB\")\n",
    "session = Session(engine)\n",
    "\n",
    "print(f\"Using {DEVICE=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def load_labeled_data(engine):\n",
    "    with Session(engine) as session:\n",
    "        samples = (\n",
    "            session.query(bm.Sample)\n",
    "            .options(joinedload(bm.Sample.raw_labels))\n",
    "            .filter(bm.Sample.used_in_batch != None)\n",
    "        )\n",
    "\n",
    "    rows = []\n",
    "    for sample in samples.all():\n",
    "        if sample.n_coders > 0:\n",
    "            row = (sample.id, sample.text, sample.labels())\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"id\", \"text\", \"labels\"])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_unlabeled_data(engine):\n",
    "    with Session(engine) as session:\n",
    "        samples = session.query(bm.Sample).filter(bm.Sample.used_in_batch == None)\n",
    "\n",
    "    rows = []\n",
    "    for sample in samples.all():\n",
    "        row = (sample.id, sample.text)\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"id\", \"text\"])\n",
    "\n",
    "\n",
    "def define_labels(labels):\n",
    "    if \"antielite\" in labels:\n",
    "        return 1\n",
    "    elif \"pplcentr\" in labels:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_labeled_data(engine)\n",
    "\n",
    "df[\"label\"] = df.labels.apply(define_labels)\n",
    "\n",
    "n_non_zero = len(df[df.label != 0])\n",
    "\n",
    "zero_sample = df[df.label == 0].sample(n_non_zero)\n",
    "\n",
    "df = pd.concat([df[df.label != 0], zero_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    201\n",
       "1    172\n",
       "2     29\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\")[\"id\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure BERT model pipeline\n",
    "\n",
    "- most parts in here are taken from [this tutorial](https://nbviewer.org/github/skorch-dev/skorch/blob/master/notebooks/Hugging_Face_Finetuning.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = MAX_EPOCHS * (len(df) // BATCH_SIZE + 1)\n",
    "\n",
    "\n",
    "def lr_schedule(current_step):\n",
    "    factor = float(num_training_steps - current_step) / float(max(1, num_training_steps))\n",
    "    assert factor > 0\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModule(nn.Module):\n",
    "    def __init__(self, name, num_labels):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.reset_weights()\n",
    "\n",
    "    def reset_weights(self):\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.name, num_labels=self.num_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        pred = self.bert(**kwargs)\n",
    "        return pred.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"tokenizer\", HuggingfacePretrainedTokenizer(TOKENIZER)),\n",
    "        (\n",
    "            \"net\",\n",
    "            NeuralNetClassifier(\n",
    "                BertModule,\n",
    "                module__name=PRETRAINED_MODEL,\n",
    "                module__num_labels=df.label.nunique(),\n",
    "                optimizer=OPTMIZER,\n",
    "                lr=LR,\n",
    "                max_epochs=MAX_EPOCHS,\n",
    "                criterion=CRITERION,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                iterator_train__shuffle=True,\n",
    "                device=DEVICE,\n",
    "                callbacks=[\n",
    "                    LRScheduler(LambdaLR, lr_lambda=lr_schedule, step_every=\"batch\"),\n",
    "                    ProgressBar(),\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c8f85cbe7b4f079736670ffa687356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.8045\u001b[0m       \u001b[32m0.6790\u001b[0m        \u001b[35m0.7445\u001b[0m  16.7383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90f4c831aa47558bc53a5ebb3a7bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.3761\u001b[0m       \u001b[32m0.7160\u001b[0m        0.8394  14.8120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb2faccd6ae46308f50e401169e1ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.1688\u001b[0m       0.7037        1.2257  14.2320\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5805ba52ad144aaeb3f7b9cd94cde9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.0504\u001b[0m       0.7160        1.1249  14.4848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7149e2b12a8e469089d6bd40d2393acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.0300\u001b[0m       0.7037        1.2307  14.6253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adabc2b1c5af493dbe5fe99ab1486a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.0032\u001b[0m       \u001b[32m0.7407\u001b[0m        1.2105  14.7449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dfe8370dd14a1399a987e00cacf24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.0018\u001b[0m       0.7407        1.2673  14.3601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e25596b58643f0a20a270ad0069fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m0.0013\u001b[0m       0.7407        1.2981  14.4867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d43fbf94642460cac81d0bc51e9b105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m0.0011\u001b[0m       0.7407        1.3303  13.8690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641ce05404254b0c94211e2b52f278a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m0.0010\u001b[0m       0.7407        1.3457  14.7662\n",
      "CPU times: user 2min 12s, sys: 11.1 s, total: 2min 23s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learner = ActiveLearner(\n",
    "    estimator=pipeline,\n",
    "    X_training=df.text,\n",
    "    y_training=df.label,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get new samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ALL samples takes in insane amount of RAM in the next cell... (well over 45GB)\n",
    "# and takes forever...\n",
    "# for the real runs, the sample size can be turned up a notch (like 100_000 or so)\n",
    "X_pool = load_unlabeled_data(engine).sample(5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 s, sys: 165 ms, total: 42 s\n",
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_idx, query_sample = learner.query(X_pool.text.tolist(), n_instances=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_pool.iloc[query_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load gründl cuz it's funny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query(bm.Sample)\n",
    "    .filter(bm.Sample.id.in_(X_sample.id.tolist()))\n",
    "    .with_entities(bm.Sample.id, bm.Sample.pop_dict_score)\n",
    ")\n",
    "\n",
    "gruendl = pd.read_sql(query.statement, engine)\n",
    "gruendl = pd.merge(X_sample, gruendl, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pop_dict_score\n",
       "False    494\n",
       "True       6\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruendl.groupby(\"pop_dict_score\")[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>pop_dict_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>703145</td>\n",
       "      <td>Schon die letzte Große Koalition - ihre Regierungszeit ist schon länger her - hatte sich vorgenommen, einen solchen Gesetzentwurf zu verabschieden.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>422705</td>\n",
       "      <td>Ich hoffe, dass all diese Vorarbeiten im Innenausschuss nicht Makulatur werden, dass wir sie nicht noch einmal leisten müssen.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>904168</td>\n",
       "      <td>Wir können aber, so wie es die Bundeskanzlerin und die Regierung regelmäßig tun, unsere Verbündeten und Freunde an deren Verantwortung erinnern und gleichzeitig darauf hinwirken, dieses Ziel in der Zukunft zu erreichen.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>385193</td>\n",
       "      <td>Aber die Wiedervereinigung war gut für die Menschen in Ost und West, in Gesamtdeutschland, in Europa, sie war gut für eine friedliche Entwicklung in der Welt.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>20793</td>\n",
       "      <td>Die Bundesumweltministerin hat wahrscheinlich zu viel im „Dschungelbuch“ gelesen.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>117846</td>\n",
       "      <td>In dieser Aktuellen Stunde stehen bisher nicht die belegbaren Fakten im Mittelpunkt, sondern gefühlte Wahrheiten, Spekulationen, teils auch unverschämte Unterstellungen.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>605148</td>\n",
       "      <td>Sanktionen, Dialogverbote und primitive antirussische Propaganda sind keine Argumente, sondern zerstören die Grundlagen von Zusammenarbeit.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1115709</td>\n",
       "      <td>Der sogenannte Bürgerdialog Stromnetz soll den Anwohnern also schmackhaft machen, dass sie vom Wertverlust ihres Grundstückes am Ende noch profitieren.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1226711</td>\n",
       "      <td>Wenn ich die Veräußerungsverluste ohne Veräußerung anerkenne, ist das eine Einladung an Spekulanten, zu spekulieren, weil klar ist: Den Verlust tragen alle anderen. – Wir müssen also hochsensibel sein und gemeinsam überlegen, was zu tun ist.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>503573</td>\n",
       "      <td>Unsere Soldatinnen und Soldaten haben einen außerordentlichen Beitrag für das afghanische Volk geleistet und sich für die Stabilisierung dieses Landes eingesetzt, damit eben Unterdrückung aufhört, damit das Sterben aufhört.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  \\\n",
       "31    703145   \n",
       "352   422705   \n",
       "159   904168   \n",
       "218   385193   \n",
       "299    20793   \n",
       "344   117846   \n",
       "158   605148   \n",
       "222  1115709   \n",
       "122  1226711   \n",
       "398   503573   \n",
       "\n",
       "                                                                                                                                                                                                                                                  text  \\\n",
       "31                                                                                                 Schon die letzte Große Koalition - ihre Regierungszeit ist schon länger her - hatte sich vorgenommen, einen solchen Gesetzentwurf zu verabschieden.   \n",
       "352                                                                                                                     Ich hoffe, dass all diese Vorarbeiten im Innenausschuss nicht Makulatur werden, dass wir sie nicht noch einmal leisten müssen.   \n",
       "159                        Wir können aber, so wie es die Bundeskanzlerin und die Regierung regelmäßig tun, unsere Verbündeten und Freunde an deren Verantwortung erinnern und gleichzeitig darauf hinwirken, dieses Ziel in der Zukunft zu erreichen.   \n",
       "218                                                                                     Aber die Wiedervereinigung war gut für die Menschen in Ost und West, in Gesamtdeutschland, in Europa, sie war gut für eine friedliche Entwicklung in der Welt.   \n",
       "299                                                                                                                                                                  Die Bundesumweltministerin hat wahrscheinlich zu viel im „Dschungelbuch“ gelesen.   \n",
       "344                                                                          In dieser Aktuellen Stunde stehen bisher nicht die belegbaren Fakten im Mittelpunkt, sondern gefühlte Wahrheiten, Spekulationen, teils auch unverschämte Unterstellungen.   \n",
       "158                                                                                                        Sanktionen, Dialogverbote und primitive antirussische Propaganda sind keine Argumente, sondern zerstören die Grundlagen von Zusammenarbeit.   \n",
       "222                                                                                            Der sogenannte Bürgerdialog Stromnetz soll den Anwohnern also schmackhaft machen, dass sie vom Wertverlust ihres Grundstückes am Ende noch profitieren.   \n",
       "122  Wenn ich die Veräußerungsverluste ohne Veräußerung anerkenne, ist das eine Einladung an Spekulanten, zu spekulieren, weil klar ist: Den Verlust tragen alle anderen. – Wir müssen also hochsensibel sein und gemeinsam überlegen, was zu tun ist.   \n",
       "398                    Unsere Soldatinnen und Soldaten haben einen außerordentlichen Beitrag für das afghanische Volk geleistet und sich für die Stabilisierung dieses Landes eingesetzt, damit eben Unterdrückung aufhört, damit das Sterben aufhört.   \n",
       "\n",
       "     pop_dict_score  \n",
       "31            False  \n",
       "352           False  \n",
       "159           False  \n",
       "218           False  \n",
       "299           False  \n",
       "344            True  \n",
       "158            True  \n",
       "222            True  \n",
       "122            True  \n",
       "398            True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruendl.groupby(\"pop_dict_score\").sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export new batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = src.PATH / \"tmp\"\n",
    "tmpdir.mkdir(exist_ok=True)\n",
    "X_sample.to_parquet(tmpdir / \"active_learning_batch.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "05b0f58c2bcdc36936de25f1aaac0724cceaad7ddc88039a552f8d696c8fb19b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
