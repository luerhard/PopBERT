{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from src.bert.dataset import PBertDataset\n",
    "from src.bert.dataset.strategies import MLMin1PopIdeol\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import src\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tmp/PopBERT_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PBertDataset.from_disk(\n",
    "    path=src.PATH / \"data/bert/test.csv.zip\",\n",
    "    label_strategy=MLMin1PopIdeol(),\n",
    "    exclude_coders=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = test.create_collate_fn(tokenizer)\n",
    "test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "THRESHOLDS = {0: 0.37128925, 1: 0.20994559, 2: 0.47641852, 3: 0.20014147}\n",
    "\n",
    "def apply_thresh(y_proba, thresholds: dict):\n",
    "    y_proba = y_proba.copy()\n",
    "    for dim, thresh in thresholds.items():\n",
    "        y_proba[:, dim] = np.where(y_proba[:, dim] > thresh, 1, 0)\n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in test_loader:\n",
    "        encodings = batch[\"encodings\"]\n",
    "        encodings = encodings.to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        out = model(**encodings)\n",
    "        preds = torch.nn.functional.sigmoid(out.logits)\n",
    "        y_true.extend(batch[\"labels\"].numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "    y_pred_05 = np.where(np.array(y_pred) > 0.5, 1, 0)\n",
    "    y_pred_thresh = apply_thresh(np.array(y_pred), THRESHOLDS)\n",
    "    y_true = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.37128925, 1: 0.20994559, 2: 0.47641852, 3: 0.20014147}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.bert.utils import ThresholdFinder\n",
    "tf = ThresholdFinder()\n",
    "\n",
    "tf.find_thresholds(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       elite       0.82      0.86      0.84       648\n",
      "    pplcentr       0.77      0.60      0.67       322\n",
      "        left       0.71      0.75      0.73       279\n",
      "       right       0.77      0.57      0.65       155\n",
      "\n",
      "   micro avg       0.78      0.75      0.76      1404\n",
      "   macro avg       0.77      0.69      0.72      1404\n",
      "weighted avg       0.78      0.75      0.76      1404\n",
      " samples avg       0.39      0.38      0.38      1404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme_storage/git/bert_populism/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/nvme_storage/git/bert_populism/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_05, target_names=[\"elite\", \"pplcentr\", \"left\", \"right\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       elite       0.81      0.88      0.84       648\n",
      "    pplcentr       0.70      0.73      0.71       322\n",
      "        left       0.69      0.77      0.73       279\n",
      "       right       0.68      0.66      0.67       155\n",
      "\n",
      "   micro avg       0.75      0.80      0.77      1404\n",
      "   macro avg       0.72      0.76      0.74      1404\n",
      "weighted avg       0.75      0.80      0.77      1404\n",
      " samples avg       0.41      0.40      0.40      1404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme_storage/git/bert_populism/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/nvme_storage/git/bert_populism/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "THRESHOLDS = {0: 0.415961, 1: 0.295400, 2: 0.429109, 3: 0.302714}\n",
    "y_pred_thresh = apply_thresh(np.array(y_pred), THRESHOLDS)\n",
    "print(classification_report(y_true, y_pred_thresh, target_names=[\"elite\", \"pplcentr\", \"left\", \"right\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_populism",
   "language": "python",
   "name": "bert_populism"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
